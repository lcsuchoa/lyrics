---
title: "Classificação de emoções e sentimentos em letras musicais usando Floresta Aleatória e Otimização Bayesiana"
date: "Última atualização em `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: simplex
    css: src/style.css
    toc: true
    toc_float:
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

```{r echo=F, include=F}
source("src/utils.R")
```

## Leitura dos dados

```{r}
data_songs <- read.csv("src/all_songs.csv") %>%
  select(-X) %>%
  mutate(
    lyrics = Slyrics %>%
      gsub("\\d+", "", .) %>%
      tolower() %>%
      gsub("[[:punct:]]", "", .) %>%
      remove_stopwords() %>%
      as.character()
  )

data_artists <- read.csv("src/all_artists.csv")
```

```{r warning=F}
data <- read.csv("src/data_gabarito.csv")

gabarito <- data[1,] %>%
  t() %>%
  as.data.frame() %>%
  rename(sentimentos = 1) %>%
  mutate(
    anger = ifelse(grepl("Raiva", sentimentos), 1, 0),
    anticipation = ifelse(grepl("Antecipação", sentimentos), 1, 0),
    disgust = ifelse(grepl("Desgosto", sentimentos), 1, 0),
    fear = ifelse(grepl("Medo", sentimentos), 1, 0),
    joy = ifelse(grepl("Alegria", sentimentos), 1, 0),
    sadness = ifelse(grepl("Tristeza", sentimentos), 1, 0),
    surprise = ifelse(grepl("Surpresa", sentimentos), 1, 0),
    trust = ifelse(grepl("Confiança", sentimentos), 1, 0),
    negative = ifelse(grepl("Negativo", sentimentos), 1, 0),
    positive = ifelse(grepl("Positivo", sentimentos), 1, 0),
  ) %>%
  select(-sentimentos) %>%
  mutate(across(everything(), as.factor))

row.names(gabarito) <- NULL
data_gabarito <- cbind(data_songs, gabarito) %>%
  mutate(
    across(anger:positive, ~ as.numeric(as.character(.))),
    lyrics = data_songs$lyrics
  ) %>%
  filter(Alink != "/hillsong-united/") %>%
  distinct(Slink, .keep_all = TRUE)

data_songs <- data_songs %>%
  filter(Alink != "/hillsong-united/") %>%
  distinct(Slink, .keep_all = TRUE)

rm(data, gabarito)
```


## Análise exploratória

##### Frequência de músicas anotadas por sentimento
```{r}
data_gabarito %>%
  reframe(across(anger:positive, sum)) %>%
  pivot_longer(cols = everything(), names_to = "sentimento", values_to = "count") %>%
  mutate(sentimento = labels_pt[sentimento]) %>%
  arrange(desc(count)) %>%
  mutate(sentimento = factor(sentimento, levels = sentimento)) %>%
  ggplot(aes(x = sentimento, y = count)) +
  geom_bar(stat = "identity", fill = "darkblue") +
  geom_text(aes(label = count), vjust = 2, size = 3, color = "white") + 
  scale_y_continuous(limits = c(0, 300)) +
  theme_minimal() +
  labs(x = "", y = "Frequência", title = "") +
  theme(axis.text.x = element_text())
```

##### Matriz de correlação entre sentimentos
```{r}
correlation_matrix <- cor(data_gabarito %>% select(anger:positive))
hc <- hclust(dist(1 - correlation_matrix))
ordered_matrix <- correlation_matrix[hc$order, hc$order]
long_correlation_matrix <- melt(ordered_matrix)

long_correlation_matrix %>%
  ggplot(aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(name = "Correlação",  mid = "white",  high = "darkblue",
                       low = "darkorange",  midpoint = 0, limit = c(-1, 1), 
                       breaks = c(-1, -0.5, 0, 0.5, 1)) +
  theme_minimal() +
  labs(x = "", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), 
        axis.text.y = element_text(hjust = 1)) +
  scale_x_discrete(labels = labels_pt) +
  scale_y_discrete(labels = labels_pt)

rm(correlation_matrix, hc, ordered_matrix, long_correlation_matrix)
```

##### Proporção de sentimentos por gênero
```{r}
data_gabarito %>%
  left_join(data_artists, by="Alink") %>%
  select(-c(X, Asongs, Apopularity)) %>%
  distinct() %>%
  separate_rows(Agenres, sep = ";\\s*") %>%
  group_by(Agenres) %>%
  reframe(
    n = n(),
    across(anger:positive, mean)
  ) %>%
  mutate(
    total = anger+anticipation+disgust+fear+joy+sadness+surprise+trust+negative+positive,
    across(anger:positive, ~ .x / total)
  ) %>%
  filter(n >= 50) %>%
  arrange(desc(n)) %>%
  pivot_longer(
    cols = anger:positive,
    names_to = "sentiment",
    values_to = "percentage"
  ) %>%
  mutate(sentiment = factor(sentiment, levels = c("positive", "joy", "trust", "surprise", "anticipation", "fear", "anger", "sadness", "disgust", "negative"))) %>%
  ggplot(aes(x = Agenres, y = percentage, fill = sentiment)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)), 
            position = position_stack(vjust = 0.5), 
            color = "white", 
            size = 3) +
  scale_fill_manual(values = colorRampPalette(colors = c("darkblue", "darkgrey", "darkorange"))(10), labels = labels_pt) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Gênero", y = "Percentual do Sentimento", fill = "Sentimento") +
  theme_minimal()
```


## Processamento de dados

### TF IDF
```{r}
tfidf <- DocumentTermMatrix(data_gabarito$lyrics) %>% 
  weightTfIdf() %>%
  as.matrix() %>%
  as.data.frame()

data_tfidf_raw <- cbind(tfidf, data_gabarito %>% select(anger:positive))
data_tfidf_raw <- data_tfidf_raw %>%
  mutate(across((ncol(data_tfidf_raw)-9):ncol(data_tfidf_raw), ~ as.factor(ifelse(. >= 1, "Yes", "No"))))
```

### Frequência de músicas nas palavras
```{r}
data_tfidf_raw %>%
  select(1:(ncol(data_tfidf_raw)-10)) %>%
  reframe(across(everything(), ~ sum(. > 0))) %>%
  pivot_longer(cols = everything(), names_to = "word", values_to = "frequency") %>%
  count(frequency)
```

### Limpeza de palavras raras
```{r}
useful_words <- data_tfidf_raw %>%
  select(1:(ncol(data_tfidf_raw)-10)) %>%
  reframe(across(everything(), ~ sum(. > 0))) %>%
  pivot_longer(cols = everything(), names_to = "word", values_to = "frequency") %>%
  filter(frequency >= 3) %>%
  pull(word)

data_tfidf <- data_tfidf_raw %>%
  select(all_of(useful_words), matches(paste0("^", labels_en, "$", collapse = "|")))
```

### Frequência de palavras nas músicas
```{r}
data_tfidf_raw %>%
  select(all_of(useful_words)) %>%
  rowwise() %>%
  mutate(word_count = sum(c_across(everything()) > 0)) %>%
  ungroup() %>%
  select(word_count) %>%
  count(word_count) %>%
  arrange(desc(n))
```

### Palavras mais frequentes
```{r}
data_tfidf %>%
  select(1:(ncol(data_tfidf)-10)) %>%
  reframe(across(everything(), ~ sum(. > 0))) %>%
  pivot_longer(cols = everything(), names_to = "word", values_to = "frequency") %>%
  arrange(desc(frequency))
```

### Exemplo É o amor
```{r}
data_tfidf[364,1:971] %>%
  pivot_longer(cols = everything(), names_to = "word", values_to = "tfidf") %>%
  filter(tfidf > 0) %>%
  arrange(desc(tfidf))
```

### Divisão treino e teste
```{r}
set.seed(7)

data_split <- initial_split(data_tfidf, prop = 0.8)
data_train <- training(data_split)
data_test <- testing(data_split)
```


## Positivo

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(positive)
model_data <- bind_cols(tfidf_cols, response_col)

# data_split <- initial_split(model_data, prop = 0.8, strata = positive)
# train_data <- training(data_split)
# test_data <- testing(data_split)

rf_recipe <- recipe(positive ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(positive) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = data_train)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

# importance <- final_model$variable.importance
# importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
#   arrange(desc(importance))
# row.names(importance_df) <- NULL
# print(importance_df %>% head(100))
# 
# test_predictions <- predict(final_fit, test_data, type = "prob") %>%
#   bind_cols(test_data)
# 
# roc_auc_result <- test_predictions %>%
#   roc_auc(truth = positive, .pred_Yes, event_level = "second")
# print(roc_auc_result)
# 
# class_predictions <- predict(final_fit, test_data) %>%
#   bind_cols(test_data)
# 
# metrics <- metric_set(accuracy, precision, recall, f_meas)
# results <- class_predictions %>%
#   metrics(truth = positive, estimate = .pred_class)
# print(results)
# 
# class_predictions %>%
#   conf_mat(truth = positive, estimate = .pred_class) %>%
#   autoplot(type = "heatmap") +
#   labs(
#     title = "Matriz de Confusão",
#     x = "Verdadeiro",
#     y = "Predito")
# 
# test_predictions %>%
#   roc_curve(truth = positive, .pred_Yes, event_level = "second") %>%
#   ggplot(aes(x = 1-specificity, y = sensitivity)) +
#   geom_path() +
#   geom_abline(lty = 3) +
#   coord_equal() +
#   labs(title = "",
#        x = "1 - Especificidade",
#        y = "Sensibilidade") +
#   theme_bw()

toc()
```

```{r}
values = NULL
for(i in 1:20) {
  set.seed(i)
  final_fit <- fit(rf_wf, data = data_train)
  final_model <- extract_fit_parsnip(final_fit)$fit
  
  test_predictions <- predict(final_fit, data_test, type = "prob") %>%
    bind_cols(data_test)
  
  roc_auc_result <- test_predictions %>%
    roc_auc(truth = positive, .pred_Yes, event_level = "second")
  roc_values[i] <- roc_auc_result$.estimate
}
cat("Melhor ROC AUC:", max(roc_values), "está no índice", which.max(roc_values))
```

```{r}
set.seed(9)
final_fit <- fit(final_wf, data = data_train)
final_model <- extract_fit_parsnip(final_fit)$fit

test_predictions <- predict(final_fit, data_test, type = "prob") %>%
  bind_cols(data_test)

cat("ROC AUC:", max(roc_values) %>% round(4), "\n")

res <- pROC::roc(test_predictions$positive, test_predictions$.pred_Yes)
value <- res$thresholds[which.max(res$sensitivities^2 + res$specificities^2)]
cat("Acurácia:", (ifelse(test_predictions$.pred_Yes > value, 1, 0) %>% sum() / nrow(test_predictions)) %>% round(4), "\n")

test_predictions_final <- test_predictions %>%
  mutate(
    pred = ifelse(test_predictions$.pred_Yes > value, 1, 0) %>% as.factor(),
    positive = ifelse(positive == "Yes", 1, 0) %>% as.factor()
  ) %>% select(pred, positive)

confusion_matrix <- confusionMatrix(test_predictions_final$pred, test_predictions_final$positive)

tp <- confusion_matrix$table[1, 1]
fn <- confusion_matrix$table[2, 1]
fp <- confusion_matrix$table[1, 2]
sensibilidade <- tp / (tp + fn)
precisao <- tp / (tp + fp)
f_measure <- 2 * (precisao * sensibilidade) / (precisao + sensibilidade)

cat("Sensibilidade:", sensibilidade %>% round(4), "\n")
cat("Precisão:", precisao %>% round(4), "\n")
cat("F-Measure:", f_measure %>% round(4), "\n")

test_predictions %>%
  roc_curve(truth = positive, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(positive)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = positive)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(positive ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(positive) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = positive), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = positive, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = positive, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = positive, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = positive, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = positive, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = positive, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = positive, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = positive, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(positive)
model_data <- bind_cols(tfidf_cols, response_col)

# data_split <- initial_split(model_data, prop = 0.8, strata = positive)
# train_data <- training(data_split)
# test_data <- testing(data_split)

rf_recipe <- recipe(positive ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(positive) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(1, 100)),
  min_n(range = c(5, 20))
  # num_comp(range = c(20, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(model_data, v = 5, strata = positive), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 100,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE, no_improve = 70)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

# final_fit <- fit(final_wf, data = model_data)
# final_model <- extract_fit_parsnip(final_fit)$fit
# print(final_model)
# 
# autoplot(tune_results, type = "performance")
# 
# importance <- final_model$variable.importance
# importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
#   arrange(desc(importance))
# row.names(importance_df) <- NULL
# print(importance_df %>% head(100))
# 
# test_predictions <- predict(final_fit, test_data, type = "prob") %>%
#   bind_cols(test_data)
# 
# roc_auc_result <- test_predictions %>%
#   roc_auc(truth = positive, .pred_Yes, event_level = "second")
# print(roc_auc_result)
# 
# class_predictions <- predict(final_fit, test_data) %>%
#   bind_cols(test_data)
# 
# metrics <- metric_set(accuracy, precision, recall, f_meas)
# results <- class_predictions %>%
#   metrics(truth = positive, estimate = .pred_class)
# print(results)
# 
# class_predictions %>%
#   conf_mat(truth = positive, estimate = .pred_class) %>%
#   autoplot(type = "heatmap") +
#   labs(
#     title = "Matriz de Confusão",
#     x = "Verdadeiro",
#     y = "Predito")
# 
# test_predictions %>%
#   roc_curve(truth = positive, .pred_Yes, event_level = "second") %>%
#   ggplot(aes(x = 1-specificity, y = sensitivity)) +
#   geom_path() +
#   geom_abline(lty = 3) +
#   coord_equal() +
#   labs(title = "",
#        x = "1 - Especificidade",
#        y = "Sensibilidade") +
#   theme_bw()

toc()
```

### Resultado final
```{r}
values = NULL
for(i in 1:20) {
  set.seed(i)
  final_fit <- fit(final_wf, data = data_train)
  final_model <- extract_fit_parsnip(final_fit)$fit
  
  test_predictions <- predict(final_fit, data_test, type = "prob") %>%
    bind_cols(data_test)
  
  roc_auc_result <- test_predictions %>%
    roc_auc(truth = positive, .pred_Yes, event_level = "second")
  roc_values[i] <- roc_auc_result$.estimate
}
cat("Melhor ROC AUC:", max(roc_values), "está no índice", which.max(roc_values))
```

```{r}
set.seed(8)
final_fit <- fit(final_wf, data = data_train)
final_model <- extract_fit_parsnip(final_fit)$fit

test_predictions <- predict(final_fit, data_test, type = "prob") %>%
  bind_cols(data_test)

cat("ROC AUC:", max(roc_values) %>% round(4), "\n")

res <- pROC::roc(test_predictions$positive, test_predictions$.pred_Yes)
value <- res$thresholds[which.max(res$sensitivities^2 + res$specificities^2)]
cat("Acurácia:", ifelse(test_predictions$.pred_Yes > value, 1, 0) %>% sum() / nrow(test_predictions) %>% round(4), "\n")

test_predictions_final <- test_predictions %>%
  mutate(
    pred = ifelse(test_predictions$.pred_Yes > value, 1, 0) %>% as.factor(),
    positive = ifelse(positive == "Yes", 1, 0) %>% as.factor()
  ) %>% select(pred, positive)

confusion_matrix <- confusionMatrix(test_predictions_final$pred, test_predictions_final$positive)

tp <- confusion_matrix$table[1, 1]
fn <- confusion_matrix$table[2, 1]
fp <- confusion_matrix$table[1, 2]
sensibilidade <- tp / (tp + fn)
precisao <- tp / (tp + fp)
f_measure <- 2 * (precisao * sensibilidade) / (precisao + sensibilidade)

cat("Sensibilidade:", sensibilidade %>% round(4), "\n")
cat("Precisão:", precisao %>% round(4), "\n")
cat("F-Measure:", f_measure %>% round(4), "\n")

test_predictions %>%
  roc_curve(truth = positive, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


## Negativo

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(negative)
model_data <- bind_cols(tfidf_cols, response_col)

# data_split <- initial_split(model_data, prop = 0.8, strata = negative)
# train_data <- training(data_split)
# test_data <- testing(data_split)

rf_recipe <- recipe(negative ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(negative) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = data_train)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

# importance <- final_model$variable.importance
# importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
#   arrange(desc(importance))
# row.names(importance_df) <- NULL
# print(importance_df %>% head(100))
# 
# test_predictions <- predict(final_fit, test_data, type = "prob") %>%
#   bind_cols(test_data)
# 
# roc_auc_result <- test_predictions %>%
#   roc_auc(truth = negative, .pred_Yes, event_level = "second")
# print(roc_auc_result)
# 
# class_predictions <- predict(final_fit, test_data) %>%
#   bind_cols(test_data)
# 
# metrics <- metric_set(accuracy, precision, recall, f_meas)
# results <- class_predictions %>%
#   metrics(truth = negative, estimate = .pred_class)
# print(results)
# 
# class_predictions %>%
#   conf_mat(truth = negative, estimate = .pred_class) %>%
#   autoplot(type = "heatmap") +
#   labs(
#     title = "Matriz de Confusão",
#     x = "Verdadeiro",
#     y = "Predito")
# 
# test_predictions %>%
#   roc_curve(truth = negative, .pred_Yes, event_level = "second") %>%
#   ggplot(aes(x = 1-specificity, y = sensitivity)) +
#   geom_path() +
#   geom_abline(lty = 3) +
#   coord_equal() +
#   labs(title = "",
#        x = "1 - Especificidade",
#        y = "Sensibilidade") +
#   theme_bw()

toc()
```

### Resultado final
```{r}
values = NULL
for(i in 1:20) {
  set.seed(i)
  final_fit <- fit(rf_wf, data = data_train)
  final_model <- extract_fit_parsnip(final_fit)$fit
  
  test_predictions <- predict(final_fit, data_test, type = "prob") %>%
    bind_cols(data_test)
  
  roc_auc_result <- test_predictions %>%
    roc_auc(truth = negative, .pred_Yes, event_level = "second")
  roc_values[i] <- roc_auc_result$.estimate
}
cat("Melhor ROC AUC:", max(roc_values), "está no índice", which.max(roc_values))
```

```{r}
set.seed(16)
final_fit <- fit(final_wf, data = data_train)
final_model <- extract_fit_parsnip(final_fit)$fit

test_predictions <- predict(final_fit, data_test, type = "prob") %>%
  bind_cols(data_test)

cat("ROC AUC:", max(roc_values) %>% round(4), "\n")

res <- pROC::roc(test_predictions$negative, test_predictions$.pred_Yes)
value <- res$thresholds[which.max(res$sensitivities^2 + res$specificities^2)]
cat("Acurácia:", (ifelse(test_predictions$.pred_Yes > value, 1, 0) %>% sum() / nrow(test_predictions)) %>% round(4), "\n")

test_predictions_final <- test_predictions %>%
  mutate(
    pred = ifelse(test_predictions$.pred_Yes > value, 1, 0) %>% as.factor(),
    negative = ifelse(negative == "Yes", 1, 0) %>% as.factor()
  ) %>% select(pred, negative)

confusion_matrix <- confusionMatrix(test_predictions_final$pred, test_predictions_final$negative)
print(confusion_matrix$table)

tp <- confusion_matrix$table[1, 1]
fn <- confusion_matrix$table[2, 1]
fp <- confusion_matrix$table[1, 2]
sensibilidade <- tp / (tp + fn)
precisao <- tp / (tp + fp)
f_measure <- 2 * (precisao * sensibilidade) / (precisao + sensibilidade)

cat("Sensibilidade:", sensibilidade %>% round(4), "\n")
cat("Precisão:", precisao %>% round(4), "\n")
cat("F-Measure:", f_measure %>% round(4), "\n")

test_predictions %>%
  roc_curve(truth = negative, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(negative)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = negative)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(negative ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(negative) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = negative), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = negative, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = negative, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = negative, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = negative, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = negative, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = negative, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = negative, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = negative, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(negative)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = negative)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(negative ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(negative) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = negative), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = negative, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = negative, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = negative, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = negative, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = negative, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = negative, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = negative, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = negative, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


## Alegria

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(joy)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = joy)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(joy ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(joy) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 99,
  trees = 313,
  min_n = 12
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = joy, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = joy, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = joy, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = joy, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = joy, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = joy, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = joy, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = joy, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(joy)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = joy)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(joy ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(joy) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = joy), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = joy, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = joy, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = joy, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = joy, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = joy, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = joy, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = joy, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = joy, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(joy)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = joy)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(joy ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(joy) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = joy), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = joy, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = joy, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = joy, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = joy, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = joy, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = joy, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = joy, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = joy, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


## Antecipação

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(anticipation)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = anticipation)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(anticipation ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(anticipation) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anticipation, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anticipation, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anticipation, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anticipation, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anticipation, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anticipation, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anticipation, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anticipation, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(anticipation)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = anticipation)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(anticipation ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(anticipation) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = anticipation), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anticipation, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anticipation, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anticipation, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anticipation, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anticipation, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anticipation, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anticipation, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anticipation, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(anticipation)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = anticipation)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(anticipation ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(anticipation) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = anticipation), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anticipation, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anticipation, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anticipation, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anticipation, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anticipation, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anticipation, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anticipation, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anticipation, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```



## Confiança

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(trust)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = trust)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(trust ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(trust) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = trust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = trust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = trust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = trust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = trust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = trust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = trust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = trust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(trust)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = trust)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(trust ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(trust) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = trust), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = trust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = trust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = trust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = trust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = trust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = trust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = trust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = trust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(trust)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = trust)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(trust ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(trust) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = trust), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = trust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = trust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = trust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = trust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = trust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = trust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = trust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = trust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


## Desgosto

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(disgust)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = disgust)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(disgust ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(disgust) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = disgust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = disgust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = disgust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = disgust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = disgust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = disgust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = disgust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = disgust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(disgust)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = disgust)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(disgust ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(disgust) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = disgust), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = disgust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = disgust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = disgust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = disgust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = disgust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = disgust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = disgust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = disgust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(disgust)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = disgust)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(disgust ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(disgust) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = disgust), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = disgust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = disgust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = disgust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = disgust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = disgust, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = disgust, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = disgust, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = disgust, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


## Medo

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(fear)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = fear)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(fear ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(fear) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = fear, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = fear, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = fear, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = fear, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = fear, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = fear, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = fear, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = fear, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(fear)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = fear)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(fear ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(fear) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = fear), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = fear, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = fear, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = fear, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = fear, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = fear, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = fear, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = fear, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = fear, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(fear)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = fear)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(fear ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(fear) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = fear), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = fear, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = fear, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = fear, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = fear, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = fear, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = fear, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = fear, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = fear, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


## Raiva

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(anger)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = anger)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(anger ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(anger) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anger, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anger, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anger, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anger, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anger, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anger, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anger, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anger, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(anger)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = anger)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(anger ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(anger) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = anger), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anger, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anger, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anger, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anger, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anger, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anger, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anger, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anger, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(anger)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = anger)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(anger ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(anger) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = anger), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anger, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anger, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anger, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anger, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = anger, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = anger, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = anger, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = anger, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

## Surpresa

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(surprise)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = surprise)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(surprise ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(surprise) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = surprise, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = surprise, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = surprise, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = surprise, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = surprise, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = surprise, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = surprise, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = surprise, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(surprise)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = surprise)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(surprise ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(surprise) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = surprise), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = surprise, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = surprise, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = surprise, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = surprise, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = surprise, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = surprise, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = surprise, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = surprise, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(surprise)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = surprise)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(surprise ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(surprise) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = surprise), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = surprise, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = surprise, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = surprise, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = surprise, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = surprise, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = surprise, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = surprise, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = surprise, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```



## Tristeza

### RF | Manual
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en))) %>% scale()
response_col <- data_train %>% select(sadness)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = sadness)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(sadness ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(sadness) %>%
  step_pca(all_predictors(), num_comp = 100)

rf_model <- rand_forest(
  mtry = 7,
  trees = 100,
  min_n = 50
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

final_fit <- fit(rf_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = sadness, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = sadness, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = sadness, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = sadness, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = sadness, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = sadness, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = sadness, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = sadness, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

### RF | Tune Grid
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(sadness)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = sadness)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(sadness ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(sadness) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- grid_regular(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100)),
  levels = 5
)


set.seed(7)
tune_results <- tune_grid(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = sadness), # Validação cruzada estratificada
  grid = rf_grid,
  metrics = metric_set(yardstick::roc_auc),
  control = control_grid(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = sadness, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = sadness, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = sadness, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = sadness, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = sadness, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = sadness, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = sadness, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = sadness, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```


### RF | Otimização Bayesiana
```{r}
tic()
set.seed(7)

tfidf_cols <- data_train %>% select(1:(ncol(data_train)-length(labels_en)))
response_col <- data_train %>% select(sadness)
model_data <- bind_cols(tfidf_cols, response_col)

data_split <- initial_split(model_data, prop = 0.8, strata = sadness)
train_data <- training(data_split)
test_data <- testing(data_split)

rf_recipe <- recipe(sadness ~ ., data = model_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  # step_smote(sadness) %>%
  step_pca(all_predictors(), num_comp = tune())

rf_model <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_grid <- parameters(
  trees(range = c(100, 500)),
  mtry(range = c(floor(sqrt(ncol(train_data) - 1)*0.1), floor(sqrt(ncol(train_data) - 1)*1.5))),
  min_n(range = c(5, 20)),
  num_comp(range = c(5, 100))
)

set.seed(7)
tune_results <- tune_bayes(
  rf_wf,
  resamples = vfold_cv(train_data, v = 5, strata = sadness), # Validação cruzada estratificada
  param_info = rf_grid,
  initial = 10,
  iter = 50,
  metrics = metric_set(yardstick::roc_auc),
  control = control_bayes(verbose = TRUE)
)

best_results <- show_best(tune_results, metric = "roc_auc")
print(best_results)

final_wf <- finalize_workflow(
  rf_wf,
  select_best(tune_results, metric = "roc_auc")
)

final_fit <- fit(final_wf, data = train_data)
final_model <- extract_fit_parsnip(final_fit)$fit
print(final_model)

importance <- final_model$variable.importance
importance_df <- data.frame(Feature = names(importance), Importance = importance) %>%
  arrange(desc(importance))
row.names(importance_df) <- NULL
print(importance_df %>% head(100))

test_predictions <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = sadness, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = sadness, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = sadness, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = sadness, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()

toc()
```

```{r}
test_predictions <- predict(final_fit, train_data, type = "prob") %>%
  bind_cols(train_data)

roc_auc_result <- test_predictions %>%
  roc_auc(truth = sadness, .pred_Yes, event_level = "second")
print(roc_auc_result)

class_predictions <- predict(final_fit, train_data) %>%
  bind_cols(train_data)

metrics <- metric_set(accuracy, precision, recall, f_meas)
results <- class_predictions %>%
  metrics(truth = sadness, estimate = .pred_class)
print(results)

class_predictions %>%
  conf_mat(truth = sadness, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Matriz de Confusão",
    x = "Verdadeiro",
    y = "Predito")

test_predictions %>%
  roc_curve(truth = sadness, .pred_Yes, event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "",
       x = "1 - Especificidade",
       y = "Sensibilidade") +
  theme_bw()
```

```{r}
print("Fim")
```

